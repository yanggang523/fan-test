import cv2
import numpy as np
from collections import deque
from config import Gesture
from sensor.camera_stream import start_stream, read_frame
import mediapipe as mp

# Mediapipe Hands ì´ˆê¸°í™”
mp_hands = mp.solutions.hands
hands = mp_hands.Hands(
    static_image_mode=False,
    max_num_hands=2,
    min_detection_confidence=0.7
)
drawing = mp.solutions.drawing_utils

#â€” Temporal Smoothing íˆìŠ¤í† ë¦¬(ìµœê·¼ 7í”„ë ˆì„)
gesture_history = deque(maxlen=7)

#â€” íŒŒë¼ë¯¸í„° ì„¤ì •
BASE_RATIO      = 0.15   # ë™ì  ì„ê³„ê°’ ê³„ì‚° ì‹œ ë¹„ìœ¨
ANGLE_THRESHOLD = 60.0   # ê´€ì ˆ ê°ë„ ê¸°ì¤€(ë„)

def calc_dynamic_threshold(hand_landmarks, base_ratio=BASE_RATIO):
    """ì†ë°”ë‹¥ ë„ˆë¹„ ê¸°ë°˜ ë™ì  ì„ê³„ê°’ ê³„ì‚°"""
    p1 = hand_landmarks.landmark[2]   # ì—„ì§€ PIP
    p2 = hand_landmarks.landmark[17]  # ìƒˆë¼ PIP
    palm_width = np.hypot(p1.x - p2.x, p1.y - p2.y)
    return palm_width * base_ratio

def angle(a, b, c):
    """ì„¸ ì  aâ€“bâ€“c ì˜ ë‚´ê°(ë„) ê³„ì‚°"""
    v1 = np.array([a.x - b.x, a.y - b.y])
    v2 = np.array([c.x - b.x, c.y - b.y])
    cosv = np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2) + 1e-6)
    return np.degrees(np.arccos(np.clip(cosv, -1, 1)))

def is_finger_open(hand_landmarks, tip_id, pip_id, mcp_id, angle_thresh=ANGLE_THRESHOLD, coord_thresh=0.0):
    """
    ê´€ì ˆ ê°ë„ ë˜ëŠ” ì¢Œí‘œ ì°¨ì´ ë‘˜ ì¤‘ í•˜ë‚˜ë¼ë„ ê¸°ì¤€ ì´ìƒì´ë©´ 'í´ì§'ìœ¼ë¡œ ê°„ì£¼
    coord_thresh: (pip.y - tip.y) ê°™ì€ ì¢Œí‘œ ì°¨ì´ ì„ê³„ê°’
    """
    lm_tip = hand_landmarks.landmark[tip_id]
    lm_pip = hand_landmarks.landmark[pip_id]
    lm_mcp = hand_landmarks.landmark[mcp_id]

    ang = angle(lm_mcp, lm_pip, lm_tip)
    coord = (lm_pip.y - lm_tip.y)
    return (ang > angle_thresh) or (coord > coord_thresh)

def get_finger_states(hand_landmarks, handedness):
    """
    5ê°œ ì†ê°€ë½ ìƒíƒœ(True=í´ì§, False=êµ¬ë¶€ë¦¼) ë°˜í™˜
    ì—„ì§€ëŠ” x ì¢Œí‘œ ì°¨ì´ + ê°ë„, ë‚˜ë¨¸ì§€ëŠ” angle + ë™ì  coord_thresh ì‚¬ìš©
    """
    dynamic_thresh = calc_dynamic_threshold(hand_landmarks)
    is_right = handedness.classification[0].label == "Right"

    states = []
    # ì—„ì§€: TIP=4, PIP=2, MCP=2 (PIP ëŒ€ì‹  MCPë¡œ angle ê³„ì‚°)
    lm_tip, lm_pip = hand_landmarks.landmark[4], hand_landmarks.landmark[2]
    coord_thresh_thumb = dynamic_thresh
    # ì—„ì§€ ì¢Œí‘œ ì°¨ì´
    coord_thumb = (lm_tip.x - lm_pip.x) if is_right else (lm_pip.x - lm_tip.x)
    # ì—„ì§€ ê°ë„: MCP(2)-PIP(2)-TIP(4)
    ang_thumb = angle(hand_landmarks.landmark[1],  # MCP
                    hand_landmarks.landmark[2],  # PIP
                    hand_landmarks.landmark[4])
    states.append((coord_thumb > coord_thresh_thumb) or (ang_thumb > ANGLE_THRESHOLD))

    # ê²€ì§€~ì†Œì§€
    TIP_IDS = [8, 12, 16, 20]
    PIP_IDS = [6, 10, 14, 18]
    MCP_IDS = [5, 9, 13, 17]
    for tip, pip, mcp in zip(TIP_IDS, PIP_IDS, MCP_IDS):
        states.append(is_finger_open(
            hand_landmarks,
            tip_id=tip,
            pip_id=pip,
            mcp_id=mcp,
            angle_thresh=ANGLE_THRESHOLD,
            coord_thresh=dynamic_thresh
        ))
    return states  # [thumb, index, middle, ring, pinky]

def is_thumb_down(hand_landmarks):
    tip = hand_landmarks.landmark[4]
    mcp = hand_landmarks.landmark[2]
    return (tip.y > mcp.y) and (abs(tip.x - mcp.x) < 0.2)

def classify_gesture_from_states(states, hand_landmarks):
    """
    states = [thumb, index, middle, ring, pinky]
    ì—„ì§€ë§Œ íˆì„ ë•Œ:
      - thumbs up: ì—„ì§€ TIP.y < min(ë‚˜ë¨¸ì§€ 4 fingers TIP.y)
      - thumbs down: ì—„ì§€ TIP.y > max(ë‚˜ë¨¸ì§€ 4 fingers TIP.y)
    """
    # ì˜¤ì§ ì—„ì§€ë§Œ íˆëŠ”ì§€ í™•ì¸
    if states[0] and not any(states[1:]):
        tips = hand_landmarks.landmark
        thumb_y = tips[4].y
        other_ys = [tips[i].y for i in (8,12,16,20)]
        
        if thumb_y < min(other_ys):
            return Gesture.THUMBS_UP
        if thumb_y > max(other_ys):
            return Gesture.THUMBS_DOWN

    # ëª¨ë‘ ì ‘í˜”ì„ ë•Œë§Œ ì™„ì „ ì£¼ë¨¹(FIST)
    if sum(states) == 0:
        return Gesture.FIST

    return None

def smooth_gesture(raw):
    """íˆìŠ¤í† ë¦¬ ê¸°ë°˜ ë‹¤ìˆ˜ê²°+ì—°ì† 3íšŒ ì•ˆì •í™”"""
    gesture_history.append(raw)
    most_common = max(set(gesture_history), key=gesture_history.count)
    if most_common is not None and gesture_history.count(most_common) >= 3:
        return most_common
    return None

def detect_gesture(ffmpeg_proc, debug=False):
    frame = read_frame(ffmpeg_proc)
    if frame is None:
        return None

    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    results = hands.process(rgb)
    if not results.multi_hand_landmarks:
        return None

    raw_gesture = None
    for i, (lm, handedness) in enumerate(zip(results.multi_hand_landmarks, results.multi_handedness)):
        label = handedness.classification[0].label
        states = get_finger_states(lm, handedness)

        # ğŸ‘‹ PALM ì—„ê²© ì¸ì‹: 5ê°œ ëª¨ë‘ ì—´ë ¤ì•¼
        if all(states):
            candidate = Gesture.PALM
        else:
            candidate = classify_gesture_from_states(states, lm)

        raw_gesture = candidate or raw_gesture

        # ë¡œê·¸
        print(f"[INFO] ì†[{i}] ë°©í–¥={label}, ìƒíƒœ={states}, í›„ë³´ì œìŠ¤ì²˜={candidate}")

        if debug:
            drawing.draw_landmarks(frame, lm, mp_hands.HAND_CONNECTIONS)
            cv2.putText(frame, f"{label}:{states}",
                        (10, 30 + 30*i),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255,255,255), 2)

    if debug:
        # cv2.imshow("Gesture Debug", frame)
        # cv2.waitKey(1)
        pass

    # Temporal smoothing ì ìš©
    return smooth_gesture(raw_gesture)

if __name__ == "__main__":
    ffmpeg_proc, cam_proc = start_stream()
    try:
        while True:
            g = detect_gesture(ffmpeg_proc, debug=True)
            if g:
                print(f"[STABLE] í™•ì • ì œìŠ¤ì²˜: {g.name}")
                # â‡’ ì—¬ê¸°ì— ëª¨í„° ì œì–´ ë¡œì§ ì—°ê²°
    finally:
        ffmpeg_proc.terminate()
        cam_proc.terminate()
